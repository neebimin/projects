{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Suicide Watch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3: Model Optimisation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the optimisation to identify the best hyperparameters for the following models:\n",
    "\n",
    "* Multinomial Naive Bayes\n",
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Optimising Tfidf Multinomial Naive Bayes](#Optimising-Tfidf-Multinomial-Naive_Bayes)\n",
    "- [Optimising Tfidf Logistic Regression](#Optimising-Tfidf-Logistic-Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# preprocessing imports\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# modeling imports\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split (Lemmatized Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it is classification problem, we will stratify y to ensure equal split of X and y \n",
    "# in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length to confirm that train and test variables have the same length\n",
    "print('Number of rows in X_train: {}'. format(len(X_train)))\n",
    "print('Number of rows in y_train: {}'. format(len(y_train)))\n",
    "print('Number of rows in X_test: {}'. format(len(X_test)))\n",
    "print('Number of rows in y_test: {}'. format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "The GridSearchCV tool allows us to program multiple hyperparameters across our models. It will generate a model with each combination of our desired hyperparameters, and optimize the highest-scoring result.\n",
    "\n",
    "We will run a model for each of the following classifiers:\n",
    "\n",
    "* Multinomial Naive Bayes\n",
    "* K-Nearest Neighbors\n",
    "* Logistic Regression\n",
    "\n",
    "We will run two GridSearches to benchmark these models for two feature extraction techniques: CountVectorizer and TfidfVectorizer. We can use the accuracy of the results to narrow our model selection to the most effective approaches.\n",
    "\n",
    "As these models execute, the results will be displayed, then stored into a DataFrame for final comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline accuracy\n",
    "baseline = y_train.value_counts(normalize=True)[1]\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find the baseline accuracy, which is the likelihood of a post being from the suicide subreddit, by calculating the percentage of the dataset that is the target value of 1. Normalising the value counts shows the percentage, and gives a baseline accuracy of 51.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline steps for each combination of model\n",
    "# Include standard scaler for knn and logistic regression because distance are important when classifying\n",
    "steps_list_cv = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_cv = ['multi_nb + cv','knn + cv','logreg + cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_cv = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]}\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate results DataFrame for CountVectorizer\n",
    "gs_results_cv = pd.DataFrame(columns=['model','best_params','train_accuracy','test_accuracy',\n",
    "                                      'baseline_accuracy','recall', 'precision', 'f1-score'])\n",
    "gs_results_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through index of number of steps\n",
    "for i in range(len(steps_list_cv)):\n",
    "    # instantiate pipeline \n",
    "    pipe = Pipeline(steps=steps_list_cv[i])\n",
    "    # fit GridSearchCV to model and model's params\n",
    "    gs = GridSearchCV(pipe, pipe_params_cv[i], cv=3) \n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('Model: ', steps_titles_cv[i])\n",
    "    model_results['model'] = steps_titles_cv[i]\n",
    "\n",
    "    print('Best Params: ', gs.best_params_)\n",
    "    model_results['best_params'] = gs.best_params_\n",
    "\n",
    "    print(gs.score(X_train, y_train), '\\n')\n",
    "    model_results['train_accuracy'] = gs.score(X_train, y_train)\n",
    "    \n",
    "    print(gs.score(X_test, y_test), '\\n')\n",
    "    model_results['test_accuracy'] = gs.score(X_test, y_test)\n",
    "    \n",
    "    model_results['baseline_accuracy'] = baseline\n",
    "    \n",
    "    # Display the confusion matrix results showing true/false positive/negative\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp, '\\n')\n",
    "    \n",
    "    model_results['recall'] = tp/(tp+fn)\n",
    "    model_results['precision'] = tp/(tp+fp)\n",
    "    model_results['f1-score'] = 2*((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))\n",
    "\n",
    "    gs_results_cv = gs_results_cv.append(model_results, ignore_index=True)\n",
    "    pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_cv.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Multinomial NB gave the highest test accuracy, there seems to be a high overfitting of training data since the training data accuracy is much higher than the testing data accuracy for both Multinomial NB and Logistic Regression. All three models have a higher accuracy than the baseline, but only barely so for KNearestNeighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_list_tf = [ \n",
    "    [('tf',TfidfVectorizer()),('multi_nb',MultinomialNB())],\n",
    "    [('tf',TfidfVectorizer()),('scaler',StandardScaler(with_mean=False)),('knn',KNeighborsClassifier())], \n",
    "    [('tf',TfidfVectorizer()),('scaler',StandardScaler(with_mean=False)),('logreg',LogisticRegression())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_tf = ['multi_nb + tf','knn + tf','logreg + tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_tf = [\n",
    "    {\"tf__stop_words\":['english'], \"tf__ngram_range\":[(1,1),(1,2)]},\n",
    "    {\"tf__stop_words\":['english'], \"tf__ngram_range\":[(1,1),(1,2)]},\n",
    "    {\"tf__stop_words\":['english'], \"tf__ngram_range\":[(1,1),(1,2)]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate results DataFrame\n",
    "gs_results_tf = pd.DataFrame(columns=['model','best_params','train_accuracy','test_accuracy','baseline_accuracy',\n",
    "                                      'recall', 'precision', 'f1-score'])\n",
    "gs_results_tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through index of number of steps\n",
    "for i in range(len(steps_list_tf)):\n",
    "    # instantiate pipeline \n",
    "    pipe = Pipeline(steps=steps_list_tf[i])\n",
    "    # fit GridSearchCV to model and model's params\n",
    "    gs = GridSearchCV(pipe, pipe_params_tf[i], cv=3) \n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('Model: ', steps_titles_tf[i])\n",
    "    model_results['model'] = steps_titles_tf[i]\n",
    "\n",
    "    print('Best Params: ', gs.best_params_)\n",
    "    model_results['best_params'] = gs.best_params_\n",
    "\n",
    "    print(gs.score(X_train, y_train), '\\n')\n",
    "    model_results['train_accuracy'] = gs.score(X_train, y_train)\n",
    "    \n",
    "    print(gs.score(X_test, y_test), '\\n')\n",
    "    model_results['test_accuracy'] = gs.score(X_test, y_test)\n",
    "    \n",
    "    model_results['baseline_accuracy'] = baseline\n",
    "\n",
    "    # Display the confusion matrix results showing true/false positive/negative\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp, '\\n')\n",
    "    \n",
    "    model_results['recall'] = tp/(tp+fn)\n",
    "    model_results['precision'] = tp/(tp+fp)\n",
    "    model_results['f1-score'] = 2*((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))\n",
    "\n",
    "    gs_results_tf = gs_results_tf.append(model_results, ignore_index=True)\n",
    "    pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_tf.sort_values('test_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the result from CountVectorizer, Multinomial NB gave the highest test accuracy. The training data is still highly overfitted for both Multinomial NB and Logistic Regression but both still did better than the baseline. \n",
    "However, comparing the test accuracy between Multinomial NB + Tfidf Vectorizer against Multinomial NB + Count Vectorizer, the accuracy for the former is higher. \n",
    "\n",
    "CountVectorizer gives a vector with the number of times each word appears in the document. This leads to the problem of having common words that appear most of the time being weighted higher than other words that carry the topic information. Tfidf balances out the term frequency with its inverse document frequency which means that common words that occur across documents will have lower scores than when using CountVectorizer. Thus, the Tfidf is better able to identify words that are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes containing results from both vectorizations\n",
    "results_lem = pd.concat([gs_results_cv, gs_results_tf], ignore_index=True)\n",
    "results_lem.sort_values('test_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split (Stemmed Posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set feature as posts that were stemmed and y for the subreddit suicide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prep['post_stem']\n",
    "y = df_prep['suicide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length to confirm that train and test variables have the same length\n",
    "print('Number of rows in X_train: {}'. format(len(X_train)))\n",
    "print('Number of rows in y_train: {}'. format(len(y_train)))\n",
    "print('Number of rows in X_test: {}'. format(len(X_test)))\n",
    "print('Number of rows in y_test: {}'. format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "A model will be run for the following classifiers:\n",
    "\n",
    "* Multinomial Naive Bayes\n",
    "* K-Nearest Neighbors\n",
    "* Logistic Regression\n",
    "\n",
    "The results will be displayed, then stored into a DataFrame for a final comparison with the lemmatized posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline steps for each combination of model\n",
    "# Include standard scaler for knn and logistic regression because distance are important when classifying\n",
    "steps_list_cv_st = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_cv_st = ['multi_nb + cv','knn + cv','logreg + cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_cv_st = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]}\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate results DataFrame for CountVectorizer\n",
    "gs_results_cv_st = pd.DataFrame(columns=['model','best_params','train_accuracy','test_accuracy',\n",
    "                                      'baseline_accuracy','recall', 'precision', 'f1-score'])\n",
    "gs_results_cv_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through index of number of steps\n",
    "for i in range(len(steps_list_cv_st)):\n",
    "    # instantiate pipeline \n",
    "    pipe = Pipeline(steps=steps_list_cv_st[i])\n",
    "    # fit GridSearchCV to model and model's params\n",
    "    gs = GridSearchCV(pipe, pipe_params_cv_st[i], cv=3) \n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('Model: ', steps_titles_cv_st[i])\n",
    "    model_results['model'] = steps_titles_cv_st[i]\n",
    "\n",
    "    print('Best Params: ', gs.best_params_)\n",
    "    model_results['best_params'] = gs.best_params_\n",
    "\n",
    "    print(gs.score(X_train, y_train), '\\n')\n",
    "    model_results['train_accuracy'] = gs.score(X_train, y_train)\n",
    "    \n",
    "    print(gs.score(X_test, y_test), '\\n')\n",
    "    model_results['test_accuracy'] = gs.score(X_test, y_test)\n",
    "    \n",
    "    model_results['baseline_accuracy'] = baseline\n",
    "    \n",
    "    # Display the confusion matrix results showing true/false positive/negative\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp, '\\n')\n",
    "    \n",
    "    model_results['recall'] = tp/(tp+fn)\n",
    "    model_results['precision'] = tp/(tp+fp)\n",
    "    model_results['f1-score'] = 2*((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))\n",
    "    \n",
    "    gs_results_cv_st = gs_results_cv_st.append(model_results, ignore_index=True)\n",
    "    pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_cv_st.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from using stemmed data is almost the same as the results from using lemmatized data. Multinomial NB remains the best performing but with a little less overfitting than the one with lemmatized data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_list_tf_st = [ # list of pipeline steps for each model combo\n",
    "    [('tf',TfidfVectorizer()),('multi_nb',MultinomialNB())],\n",
    "    [('tf',TfidfVectorizer()),('scaler',StandardScaler(with_mean=False)),('knn',KNeighborsClassifier())], \n",
    "    [('tf',TfidfVectorizer()),('scaler',StandardScaler(with_mean=False)),('logreg',LogisticRegression())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_tf_st = ['multi_nb + tf','knn + tf','logreg + tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_tf_st = [\n",
    "    {\"tf__stop_words\":['english'], \"tf__ngram_range\":[(1,1),(1,2)]},\n",
    "    {\"tf__stop_words\":['english'], \"tf__ngram_range\":[(1,1),(1,2)]},\n",
    "    {\"tf__stop_words\":['english'], \"tf__ngram_range\":[(1,1),(1,2)]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate results DataFrame\n",
    "gs_results_tf_st = pd.DataFrame(columns=['model','best_params','train_accuracy','test_accuracy','baseline_accuracy',\n",
    "                                         'recall', 'precision', 'f1-score'])\n",
    "gs_results_tf_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through index of number of steps\n",
    "for i in range(len(steps_list_tf_st)):\n",
    "    # instantiate pipeline \n",
    "    pipe = Pipeline(steps=steps_list_tf_st[i])\n",
    "    # fit GridSearchCV to model and model's params\n",
    "    gs = GridSearchCV(pipe, pipe_params_tf_st[i], cv=3) \n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('Model: ', steps_titles_tf_st[i])\n",
    "    model_results['model'] = steps_titles_tf_st[i]\n",
    "\n",
    "    print('Best Params: ', gs.best_params_)\n",
    "    model_results['best_params'] = gs.best_params_\n",
    "\n",
    "    print(gs.score(X_train, y_train), '\\n')\n",
    "    model_results['train_accuracy'] = gs.score(X_train, y_train)\n",
    "    \n",
    "    print(gs.score(X_test, y_test), '\\n')\n",
    "    model_results['test_accuracy'] = gs.score(X_test, y_test)\n",
    "    \n",
    "    model_results['baseline_accuracy'] = baseline\n",
    "\n",
    "    # Display the confusion matrix results showing true/false positive/negative\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp, '\\n')\n",
    "    \n",
    "    model_results['recall'] = tp/(tp+fn)\n",
    "    model_results['precision'] = tp/(tp+fp)\n",
    "    model_results['f1-score'] = 2*((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))\n",
    "\n",
    "    gs_results_tf_st = gs_results_tf_st.append(model_results, ignore_index=True)\n",
    "    pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results_tf_st.sort_values('test_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial NB again is the best model with the highest accuracy but is very highly overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes containing results from both vectorizations\n",
    "results_stem = pd.concat([gs_results_cv_st, gs_results_tf_st], ignore_index=True)\n",
    "results_stem.sort_values('test_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with the above result\n",
    "results_lem.sort_values('test_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best performing model i.e. Multinomial NB with Tfidf Vectorizer, the lemmatized data performed only marginally better in terms of accuracy. However, given the context of the problem at hand, we would want to maximise recall as much as possible since it shows the model’s ability to find all the data points of interest i.e. all the suicide posts. If recall is low, it means that red flags would not be raised for those who are suicidal even when they are actually suicidal. The following models seem to do better in terms of recall and accuracy:\n",
    "1. Lemmatized Tfidf Multinomial NB\n",
    "    * tf__ngram_range=(1,1)\n",
    "    * tf__stop_words='english'\n",
    "2. Lemmatized Tfidf Scaled Logistic Regression\n",
    "    * tf__ngram_range=(1,2)\n",
    "    * tf__stop_words='english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be continued in Notebook 3: Model Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
